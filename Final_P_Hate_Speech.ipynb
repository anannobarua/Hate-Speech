{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_P_Hate_Speech.ipynb","provenance":[{"file_id":"109zSSmmA6m7j4SEGdEd7FcVx5DeN1cP1","timestamp":1619546303635},{"file_id":"17VK934hAqzoNoFbtiDG0OjLHWvoJqlMr","timestamp":1619531522843},{"file_id":"11KMnaEpQaAzNGZuG-4LDqtXkG0lRFG9h","timestamp":1619521219039},{"file_id":"15HCQFDj1_3_5vr7XN6fkkxfxqs6o6DBJ","timestamp":1619509899241},{"file_id":"1rhZjq3UorL6EU6uf1VezXNvYGE8nF6lz","timestamp":1618245832515}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofFqlqxVeL70","executionInfo":{"status":"ok","timestamp":1619796556713,"user_tz":-360,"elapsed":1265,"user":{"displayName":"1528_Ananno Barua","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzCxfWxWyaPATvsHwqI5fxPOMsk8duOu9OQDvhTg=s64","userId":"06136234510954429556"}},"outputId":"d748d438-93fc-40b5-827a-a4fc714cc9c1"},"source":["import re\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('wordnet')\n","nltk.download('words')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"kCKRvLT-eTjW"},"source":["import string\n","import inflect\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.svm import SVC\n","from nltk import pos_tag, ne_chunk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5f-KIhVeZ1z","executionInfo":{"status":"ok","timestamp":1619796565785,"user_tz":-360,"elapsed":10318,"user":{"displayName":"1528_Ananno Barua","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzCxfWxWyaPATvsHwqI5fxPOMsk8duOu9OQDvhTg=s64","userId":"06136234510954429556"}},"outputId":"ea99207b-c91e-4009-bd8c-2bcecdae9cec"},"source":["!pip install word2number\n","!pip install Unidecode\n","!pip install contractions"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: word2number in /usr/local/lib/python3.7/dist-packages (1.1)\n","Requirement already satisfied: Unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n","Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xmco37lQeYSC"},"source":["import unidecode\n","import contractions\n","from word2number import w2n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcdXJBhceYwS"},"source":["  from nltk import tag\n","  from nltk.tag import pos_tag\n","  from nltk.tree import Tree\n","  from nltk.chunk import ne_chunk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f3dFbm0oeeAU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619796565787,"user_tz":-360,"elapsed":10287,"user":{"displayName":"1528_Ananno Barua","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzCxfWxWyaPATvsHwqI5fxPOMsk8duOu9OQDvhTg=s64","userId":"06136234510954429556"}},"outputId":"783a8208-c89f-439a-c4a0-09081b838b5a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Colab Notebooks/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"drZohK5SenrE"},"source":["def read_files(file_loc):\n","  '''\n","  This function reads tsv data from a file in the drive\n","\n","  args - a string containing the files location\n","  returns - a list containing the text data\n","  '''\n","\n","  dataset = []\n","\n","  with open(file_loc, 'r', encoding='cp850') as file:\n","    for line in file:\n","      dataset.append(line)\n","\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0dS-PUOKepV5"},"source":["def separate_labels(dataset):\n","  '''This function will separate the labels/class and examples/documents from the dataset'''\n","  labels = []\n","  documents = []\n","\n","  for line in dataset:\n","    splitted_line = line.strip().split('\\t', 2)\n","    labels.append(splitted_line[2])\n","    documents.append(splitted_line[1])\n","\n","  return labels, documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TChY5hmkWw1L"},"source":["def separate_labels2(dataset):\n","  '''This function will separate the labels/class and examples/documents from the dataset'''\n","  labels = []\n","  documents = []\n","\n","  for line in dataset:\n","    splitted_line = line.strip().split('\\t', 2)\n","    labels.append(splitted_line[0])\n","    documents.append(splitted_line[1])\n","\n","  return labels, documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCrf5v3xeqyC"},"source":["def remove_url(documents):\n","  '''This function removes URL's from Texts'''\n","  url_removed = []\n","\n","  # Your code here\n","  for line in documents:\n","    url_removed.append(re.sub('http[s]?://\\S+', '', line))\n","\n","  return url_removed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJZCyib9esE7"},"source":["def remove_hashtag(documents):\n","  '''This function will remove all occurences of # from the texts'''\n","  hashtag_removed = []\n","\n","  # map hashtag to space\n","  translator = str.maketrans('#', ' '*len('#'), '')\n","\n","  for line in documents:\n","    hashtag_removed.append(line.translate(translator))\n","\n","  return hashtag_removed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7q_fmwZetSC"},"source":["def remove_whitespaces(documents):\n","  '''This function removes multiple whitespaces and replace them with a single whitespace'''\n","  whitespace_removed = []\n","\n","  for line in documents:\n","    whitespace_removed.append(' '.join(line.split()))\n","\n","  return whitespace_removed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"idduVyoJeucc"},"source":["def text_lowercasing(documents):\n","  lowercased_docs = []\n","\n","  for line in documents:\n","    lowercased_docs.append(line.lower())\n","  \n","  return lowercased_docs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LUd6o9zOev3G"},"source":["# 1- Tokenization\n","\n","def tokenization_data(documents):\n","  '''This function removes HTML's from Texts'''\n","  text_tokenized = []\n","\n","  # Your code here\n","  for line in documents:\n","    text_tokenized.append(word_tokenize(line))\n","\n","  return text_tokenized"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VRrmGFG_e17C"},"source":["def remove_punctuation(documents):\n","\n","  punct_removed = []\n","\n","  for doc in documents:\n","    temp = []\n","    for word in doc:\n","      if word not in string.punctuation:\n","        temp.append(word)\n","    \n","    punct_removed.append(temp)\n","\n","  return punct_removed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"voH1vIFme3QZ"},"source":["def remove_stopwords(documents):\n","  \n","  stopword_removed = []\n","\n","  stop_words = set(stopwords.words('english'))\n","\n","  for doc in documents:\n","    stopword_removed.append([word for word in doc if word not in stop_words])\n","    # temp = []\n","    # for word in doc:\n","    #   if word not in stop_words:\n","    #     temp.append(word)\n","    \n","    # stopword_removed.append(temp)\n","\n","  return stopword_removed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfaUvKq2e6EZ"},"source":["def char_n_gram_ready(documents):\n","  '''This function joins the tokens in a sentence like string'''\n","  joined_docs = []\n","\n","  for line in documents:\n","    joined_docs.append(' '.join(line))\n","\n","  return joined_docs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ca6ThDO3e7TJ"},"source":["def vec_tfidf(tfidf = True):\n","\n","  if tfidf:\n","    vec = TfidfVectorizer(preprocessor = identity,\n","                          tokenizer = identity)\n","  else:\n","  # vec = CountVectorizer(preprocessor = identity, analyzer='char',\n","                          # tokenizer = identity, ngram_range=(1, 3))\n","   vec = CountVectorizer(preprocessor = identity,\n","                          tokenizer = identity)\n","    \n","  return vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8895fCPGe_TR"},"source":["def SVM_Static(train_docs, train_lbls, test_docs, test_lbls):\n","  \n","  vec = vec_tfidf(tfidf=False)\n","    \n","  # combines the vectorizer with the Naive Bayes classifier\n","  classifier = Pipeline([('vec', vec),\n","                         ('cls', SVC(kernel='linear', C=1.0, gamma='scale'))])\n","  \n","  classifier.fit(train_docs, train_lbls)\n","\n","  prediction = classifier.predict(test_docs)\n","  for lbl, doc in zip(test_lbls[:10], prediction[:10]):\n","    print(lbl)\n","    print(doc)\n","    print()\n","\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzJuJbLQfAoZ"},"source":["def pre_processing(documents):\n","\n","  documents = remove_url(documents)\n","\n","\n","\n","  documents = text_lowercasing(documents)\n","\n","  \n","\n","  documents = tokenization_data(documents)\n","\n","\n","\n","  return documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWYJxYHMfO_n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619796566463,"user_tz":-360,"elapsed":10823,"user":{"displayName":"1528_Ananno Barua","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzCxfWxWyaPATvsHwqI5fxPOMsk8duOu9OQDvhTg=s64","userId":"06136234510954429556"}},"outputId":"07b3cf86-6c7b-496b-d05c-8912e2ebd2b4"},"source":["def main():\n","  print('Reading The Dataset...')\n","  \n","  # Reading the training data \n","  training_dataset = read_files('Hate_Speech/dev.txt')\n","  train_labels, train_docs = separate_labels(training_dataset)\n","  \n","\n","  # Reading the test data\n","  test_dataset = read_files('Hate_Speech/test.txt')\n","  test_labels, test_docs = separate_labels2(test_dataset)\n","\n","\n","\n","  # calling the pre processing dunction\n","  train_docs = pre_processing(train_docs)\n","  test_docs = pre_processing(test_docs)\n","  # print(train_docs)\n","\n","\n","  print('\\nTraining the SVM Classifier...')\n","  SVM_Static(train_docs, train_labels, test_docs, test_labels)\n"," \n","\n","if __name__ == '__main__':\n","  main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading The Dataset...\n","\n","Training the SVM Classifier...\n","hasoc_en_902\n","HATE\n","\n","hasoc_en_416\n","NONE\n","\n","hasoc_en_207\n","OFFN\n","\n","hasoc_en_595\n","NONE\n","\n","hasoc_en_568\n","NONE\n","\n","hasoc_en_953\n","NONE\n","\n","hasoc_en_685\n","HATE\n","\n","hasoc_en_672\n","NONE\n","\n","hasoc_en_746\n","NONE\n","\n","hasoc_en_527\n","NONE\n","\n"],"name":"stdout"}]}]}